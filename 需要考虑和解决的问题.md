- [x] 考虑安全分词：tokenizer()按特殊符号分词
- [x] test 无 label
- [x] 类增加self.dictionary字典方便调试
- [x] 解码问题：两次unquote
- [x] 考虑字典库的建立方法：单类黑样本vs自然多类全样本，参数one_class控制
- [x] pad_sequences values vs UNK，皆填充为-1
- [x] 考虑字典是否截断，参数vocabulary_size控制字典截断及大小
- [x] 单词粒度和字符粒度，参数level控制
- [x] 考虑多分类问题
- [x] 解决特征层面和模型层面的适配问题
- [x] word2vec预训练 vs keras tokenizer+wordindex+embeddings layer vs word2vec预训练+embeddings layer微调
- [ ] 扩大场景、数据，验证NLP通用安全解决方案及代码
- [x] 分词模式：无标点(concise)、全标点(all)、安全模式(define)
- [ ] 控制字典截断引发问题：不在字典中的词泛化方法不统一。wordindex默认去除or填充为某值，word2vec索引和向量默认填充为0（向量填充为UNK是否更好）